{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Q&A system powered by RAG - Integrating ChatCompletion API, Embeddings, and Pinecone\n",
        "\n",
        "In this notebook we're going to build an interactive system that lets users 'talk' with any document. Leveraging the capabilities of OpenAI's ChatCompletion API, the semantic understanding of embeddings, we'll create an application that can understand and retrieve information from documents in a conversational manner.\n"
      ],
      "metadata": {
        "id": "vUehOJlORE-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Libraries import"
      ],
      "metadata": {
        "id": "NDlbXftjSHdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTBw517ySLF2",
        "outputId": "b20ae538-427d-409b-8448-2c36934ab72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.2.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.9/219.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.1 httpx-0.25.1 openai-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pinecone-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_EmMwxN9LBg",
        "outputId": "0f7250d8-106d-4081-e845-cb4316dfd0b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0.1)\n",
            "Collecting loguru>=0.5.0 (from pinecone-client)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.5.0)\n",
            "Collecting dnspython>=2.0.0 (from pinecone-client)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
            "Installing collected packages: loguru, dnspython, pinecone-client\n",
            "Successfully installed dnspython-2.4.2 loguru-0.7.2 pinecone-client-2.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYFNyycCQ2p-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b49266-f172-44c5-b133-44fac09c40e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import PyPDF2\n",
        "import random\n",
        "import pinecone\n",
        "\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Working with PDF files\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/1*FWwgOvUE660a04zoQplS7A.png)\n",
        "\n",
        "Source: https://betterprogramming.pub/building-a-multi-document-reader-and-chatbot-with-langchain-and-chatgpt-d1864d47e339\n",
        "\n",
        "\n",
        "### 3.1 Setting up API Key"
      ],
      "metadata": {
        "id": "TwaQS_nXSQxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "btc16h6ySPFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Loading a PDF file\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wwNqZjc2th6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load a random PDF from a given directory\n",
        "def load_pdf(file_name):\n",
        "    # Read the PDF file\n",
        "    pdf_file = open(file_name, 'rb')\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "    text_content = \"\"\n",
        "    # Extract text from each page\n",
        "    for page in range(len(pdf_reader.pages)):\n",
        "        text_content += pdf_reader.pages[page].extract_text()\n",
        "\n",
        "    pdf_file.close()\n",
        "\n",
        "    return text_content"
      ],
      "metadata": {
        "id": "hV1ASHAHvm-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to chunk text by number of words or characters with a given size and overlap\n",
        "def chunk_text(text, chunk_size=1500, chunk_overlap=100, by='word'):\n",
        "    if by not in ['word', 'char']:\n",
        "        raise ValueError(\"Invalid value for 'by'. Use 'word' or 'char'.\")\n",
        "\n",
        "    chunks = []\n",
        "    if by == 'word':\n",
        "        text = text.split()\n",
        "    elif by == 'char':\n",
        "        text = text\n",
        "\n",
        "    current_chunk_start = 0\n",
        "    while current_chunk_start < len(text):\n",
        "        current_chunk_end = current_chunk_start + chunk_size\n",
        "        if by == 'word':\n",
        "            chunk = \" \".join(text[current_chunk_start:current_chunk_end])\n",
        "        else:\n",
        "            chunk = text[current_chunk_start:current_chunk_end]\n",
        "\n",
        "        chunks.append(chunk)\n",
        "        current_chunk_start += (chunk_size - chunk_overlap)\n",
        "\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "CkrkacuNwdHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_loaded = load_pdf(\"state_of_ai_docs.pdf\")"
      ],
      "metadata": {
        "id": "NrdKL3OH97rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = chunk_text(pdf_loaded, by='char')"
      ],
      "metadata": {
        "id": "LfvY_eQH97o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Building RAG system (Retrieval Augmented System)"
      ],
      "metadata": {
        "id": "NH4nGGFQyvnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(\n",
        "\tapi_key='e5ee498a-8b40-32312312',\n",
        "\tenvironment='us-west1-gcp-free'\n",
        ")\n",
        "index = pinecone.Index('rag-test')"
      ],
      "metadata": {
        "id": "kJ73xZV0yvX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(chunks)):\n",
        "    vector = client.embeddings.create(\n",
        "        model=\"text-embedding-ada-002\",\n",
        "        input=chunks[i])\n",
        "    vector = vector.data[0].embedding\n",
        "\n",
        "    upsert_response = index.upsert(\n",
        "    vectors=[\n",
        "        (\n",
        "         str(i),\n",
        "         vector,\n",
        "         {\"chunk_content\": chunks[i]}\n",
        "        )\n",
        "    ])"
      ],
      "metadata": {
        "id": "ci0oEtpzAOIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Building an interface to get proper answer based on the documentation\n"
      ],
      "metadata": {
        "id": "gdGX-w1CVxGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_request = input(\"Ask some question regarding the document: \")\n",
        "\n",
        "user_vector = client.embeddings.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=user_request)\n",
        "\n",
        "user_vector = user_vector.data[0].embedding\n",
        "matches = index.query(\n",
        "    user_vector,\n",
        "    top_k=1,\n",
        "    include_metadata=True)\n",
        "\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": \"\"\"I want you to act as a support agent. Your name is \"My Super Assistant\". You will provide me with answers from the given info. If the answer is not included, say exactly \"Ooops! I don't know that.\" and stop after that. Refuse to answer any question not about the info. Never break character and always answer on the given text.\"\"\"}]\n",
        "messages.append({\"role\": \"user\", \"content\": matches['matches'][0]['metadata']['chunk_content']})\n",
        "messages.append({\"role\": \"user\", \"content\": user_request})\n",
        "print(messages)\n",
        "chat_response = client.chat.completions.create(\n",
        "\tmodel=\"gpt-3.5-turbo\",\n",
        "\tmessages=messages,\n",
        "\ttemperature=0,\n",
        "\tmax_tokens=400,\n",
        ")\n",
        "\n",
        "messages.append({\"role\": \"assistant\", \"content\": chat_response.choices[0].message.content})\n",
        "print(\"Assistant:\", chat_response.choices[0].message.content)\n",
        "print()\n",
        "print(\"Context: \", matches['matches'][0]['metadata']['chunk_content'])\n",
        "print(\"$$$$$\")"
      ],
      "metadata": {
        "id": "SFhM14-NVUqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e921fc3a-6d85-47ea-ebf0-2500443fa5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask some question regarding the document: what are the biggest rends in AI\n",
            "[{'role': 'system', 'content': 'I want you to act as a support agent. Your name is \"My Super Assistant\". You will provide me with answers from the given info. If the answer is not included, say exactly \"Ooops! I don\\'t know that.\" and stop after that. Refuse to answer any question not about the info. Never break character and always answer on the given text.'}, {'role': 'user', 'content': 'hael Chui,  a partner at the McKinsey Global Institute and \\na partner in McKinsey’s Bay Area office, where Lareina Yee  is a senior partner; Bryce Hall,  an associate partner \\nin the Washington, DC, office; and senior partners Alex Singla  and Alexander Sukharevsky,  global leaders of \\nQuantumBlack, AI by McKinsey, based in the Chicago and London offices, respectively.\\nThey wish to thank Shivani Gupta, Abhisek Jena, Begum Ortaoglu, Barr Seitz, and Li Zhang for their contributions \\nto this work.McKinsey commentary\\nMichael Chui\\nPartner, McKinsey Global Institute\\nWe’ve been emphasizing  the importance of generative AI—and for good reason, given its \\nrevolutionizing potential—but this survey is a good reminder that there’s a lot of value out \\nthere in the broader AI world. In fact, some of our other research indicates that nongenerative \\nAI has even more value potential than generative AI. Use cases in areas such as improvements \\nin forecasting accuracy, optimizing logistics networks, and providing next-product-to-buy \\nrecommendations can all generate value for companies that can take advantage of the \\nbroader AI promise.\\nWhile reported overall AI adoption remains steady at around 55 percent, more than two-\\nthirds of respondents say their companies plan on increasing their investments in AI. And \\nwe continue to see a set of AI high performers that are building out the foundations and \\ncapabilities that allow them to generate value. One way to interpret this is that “the rich \\nare'}, {'role': 'user', 'content': 'what are the biggest rends in AI'}]\n",
            "Assistant: According to the given information, the biggest trends in AI are the emphasis on generative AI and the recognition of the value potential in non-generative AI. Additionally, the survey indicates that companies are planning to increase their investments in AI, and there is a group of AI high performers who are building the foundations and capabilities to generate value.\n",
            "\n",
            "Context:  hael Chui,  a partner at the McKinsey Global Institute and \n",
            "a partner in McKinsey’s Bay Area office, where Lareina Yee  is a senior partner; Bryce Hall,  an associate partner \n",
            "in the Washington, DC, office; and senior partners Alex Singla  and Alexander Sukharevsky,  global leaders of \n",
            "QuantumBlack, AI by McKinsey, based in the Chicago and London offices, respectively.\n",
            "They wish to thank Shivani Gupta, Abhisek Jena, Begum Ortaoglu, Barr Seitz, and Li Zhang for their contributions \n",
            "to this work.McKinsey commentary\n",
            "Michael Chui\n",
            "Partner, McKinsey Global Institute\n",
            "We’ve been emphasizing  the importance of generative AI—and for good reason, given its \n",
            "revolutionizing potential—but this survey is a good reminder that there’s a lot of value out \n",
            "there in the broader AI world. In fact, some of our other research indicates that nongenerative \n",
            "AI has even more value potential than generative AI. Use cases in areas such as improvements \n",
            "in forecasting accuracy, optimizing logistics networks, and providing next-product-to-buy \n",
            "recommendations can all generate value for companies that can take advantage of the \n",
            "broader AI promise.\n",
            "While reported overall AI adoption remains steady at around 55 percent, more than two-\n",
            "thirds of respondents say their companies plan on increasing their investments in AI. And \n",
            "we continue to see a set of AI high performers that are building out the foundations and \n",
            "capabilities that allow them to generate value. One way to interpret this is that “the rich \n",
            "are\n",
            "$$$$$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Built-in RAG system"
      ],
      "metadata": {
        "id": "FjPFdoboD1kA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload a file"
      ],
      "metadata": {
        "id": "JVZ7XXbdD8qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload a file with an \"assistants\" purpose\n",
        "file = client.files.create(\n",
        "  file=open(\"state_of_ai_mc.pdf\", \"rb\"),\n",
        "  purpose='assistants'\n",
        ")\n"
      ],
      "metadata": {
        "id": "xeAvrhQjD-GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "  instructions=\"You are a customer support chatbot. Use your knowledge base to best respond to customer queries.\",\n",
        "  model=\"gpt-4-1106-preview\",\n",
        "  tools=[{\"type\": \"retrieval\"}],\n",
        "  file_ids=[file.id]\n",
        ")"
      ],
      "metadata": {
        "id": "KEHFGKL3D0V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What are the main trends in AI?\",\n",
        "      \"file_ids\": [file.id]\n",
        "    }\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "XASwFtBXD0Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread.message = client.beta.threads.messages.create(\n",
        "  thread_id=thread.id,\n",
        "  role=\"user\",\n",
        "  content=\"I can't find in the PDF manual how to turn off this device.\",\n",
        "  file_ids=[file.id]\n",
        ")"
      ],
      "metadata": {
        "id": "q17g6FA9CLsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OZCPnJCF9Rl",
        "outputId": "0f164465-46e1-44d4-d8aa-1609e0ae7883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Thread(id='thread_hPybtjBtoF5u4xynEDoCEshw', created_at=1699516761, metadata={}, object='thread', message=ThreadMessage(id='msg_pmimC0WM2fDIEdkBMa0Ypxcf', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value=\"I can't find in the PDF manual how to turn off this device.\"), type='text')], created_at=1699516831, file_ids=['file-t5iVkofRSz2Mizh6TxwKr4Sw'], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_hPybtjBtoF5u4xynEDoCEshw'))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ho-MlviGOXy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}